## 선형 회귀 (Linear Regression)

### 단순 선형 회귀

수식: y = b0 + b1x + ε

- **선형 회귀**는 입력 변수 x, 출력 변수 y 간의 선형 관계를 모델링(출력)하는 관계.
- 매개변수와 상수 값에 따라 관계의 설명 정도가 결정.
- 상수: y축과 만나는 점의 위치 결정 (b0)
- 계수: 경사, 기울기로서 얼마나 빠르게, 어떤 기울기로 작동하는지 결정 (b1)
- 오차: ε

선형 회귀 수식에 따라 n개의 데이터를 갖고 그려 그래프(산점도, 산포도 혹은 scatter plot으로 명명)를 그린다.

### 다중 선형 회귀

수식: y = b0 + b1x1 + b2x2 + ... + bnxn

> 이때 0, 1, ..., n은 아랫첨자

- y : 예측값
- n: 특성 수
- x_i: i번째 특성 값
- b_j: j번째 모델 매개변수(편향인 b0와 가중치 b1~bn 값들 포함)

다중 선형 회귀에서의 매개변수는 정규 방정식으로 계산할 수 있다. 수식은 Week 2의 task 1 참조.

- 매개변수는 비용 함수를 최소화하는 값이다.
- y는 y^(1)부터 y^(m)까지 포함하는 타깃 벡터이다.

## 경사 하강법

**경사하강법(Gradient Descent)**은 모델 손실 함수를 최소화하기 위해 반복적으로 매개변수를 갱신하는 최적화 알고리즘

### 배치 경사 하강법

모든 데이터를 사용해 손실 함수의 기울기를 계산한 후 매개변수를 갱신한다.

### 미니배치(mini-batch) 경사 하강법

데이터를 작은 단위로 나눠 훈련. 전체 데이터가 몇 가지로 나뉘므로 각각 병렬적인 학습이 가능하나, 적절한 배치 크기를 어떻게 해야 할지에 대한 기준 실험 필요

### 확률적 경사 하강법

> SGD, Stochastic Gradient Descent

훈련 데이터 중 하나의 데이터 샘플 식을 사용하여 기울기를 계산, 매개변수를 갱신하는 방식.

## 로지스틱 회귀

분류 문제를 다루는 모델로, 시그모이드 함수를 통해 입력값을 확률로 변환하는 방식. 클래스 간 결정 경계를 찾아 데이터를 분류하는 특징을 가짐.

- 변환을 통해 특정 식의 값을 0~1 사이로 매핑시키며, 1로 만들고자 하는 것이 목표. 관심있는 부분에 대한 오차와 구분이 용이한 특성이 있다.

## SVM

### 선형 SVM

- 선형적으로 분리 가능한 데이터에 대해 사용
- 최대 마진으로 결정 경계를 정의, 클래스 간 분리 최적화

### 비선형 SVM

- 선형적으로 분리되지 않는 데이터 처리하기 위한 커널 기법 사용

> 다항식 커널, RBF(가우시안)

### SVM 회귀

- 특정 마진 내에서 최대한 데이터를 포함하도록 데이터 학습. 회귀 문제에 사용

## 결정 트리(Decision Tree)
- 데이터를 기준에 따라 분리하는 트리 구조의 모델
- 직관적, 해석이 쉬우며 다양한 데이터를 처리할 수 있으나 과적합(overfitting) 되기 쉽다.

## 랜덤 포레스트(Random Forest)
- 여러 개의 결정 트리를 결합한 앙상블 학습 방법.
- 단일 결정 트리에 비해 과적합이 덜하고 분산, 편향을 줄여 일반화 성능을 높인다.

> 앙상블이란 여러 방식의 모델을 혼합하여 더 좋은 결과를 도모하는 방식

### 작동 원리

1. 데이터 샘플링 - 훈련 데이터의 랜덤 서브셋 생성(bootstrap sampling)
2. 랜덤성 도입 - 각 트리가 서로 다른 특성 집합 사용
3. 다수결 - 모든 트리의 예측 결과를 결합해 최종 예측 (앙상블의 특성)

## 교차 검증 및 그리드 탐색

- 교차 검증(Cross Validation) - 데이터를 여러 번 나눠 모델의 일반화 성능 평가
- 그리드 탐색(Grid Search) - 여러 하이퍼파라미터 조합 시도해 최적 값 발견


## 참고자료

- 2024-1 딥러닝, 충남대학교 임성수 교수님, DSC공유대학

