# 1.SVM은 N차원을 공간을 (N-1)차원으로 나눌 수 있는 초평면을 찾는 분류 기법
SVM은 2개의 클래스를 분류할 수 있는 최적의 경계를 찾고자 합니다.
마진은 클래스들 사이의 간격, 각 클래스의 말단에 위치한 데이터들 사이의 거리를 의미합니다. SVM은 분류를 할 때 이 마진을 최대화시키고자 합니다. 즉 마진을 최대화할 수 있는 경계를 찾는 것이죠.

  ​support vector(서포트 벡터)는 마진에서 가장 가까이 위치해 있는 각 클래스의 데이터 입니다. 위 그림에서는 점선 위에 있는 두개의 데이터가 support vector입니다. 이를 서포트 벡터라고 하는 이유는 이 데이터들의 위치에 따라서 경계(초평면)의 위치가 달라지기 때문에 초평면 함수를 지지한다는 의미에서 명명되었습니다.

하드마진은 두 클래스를 분류할 수 있는 최대마진의 초평면을 찾는 방법입니다. 단, 모든 훈련데이터는 마진의 바깥족에 위치하게 선형으로 구분해야 합니다.  다시 말해서 하나의 오차도 허용하면 안된다는 것을 의미

하지만 모든 데이터를 선형으로 오차없이 나눌 수 있는 결정경계를 찾는 것은 사실상 어렵습니다. 그래서 이를 해결하고자 나온 개념이 소프트마진입니다. 

 소프트마진은 하드마진이 가진 한계를 개선하고자 나온 개념으로써 완벽하게 분류하는 초평면을 찾는 것이 아니라 어느 정도의 오분류를 허용하는 방식입니다. 소프트마진에서는 오분류를 허용하고 이를 고려하기 위해 slack variable을 사용합니다. Slack variable은 해당 결정경계로부터 잘못 분류된 데이터의 거리를 측정하기 위해 사용합니다.
 
# 2-1. 결정 트리(Decision Tree)
정의: 데이터를 기준에 따라 분리하는 트리 구조의 모델입니다.
장점: 직관적이고 해석이 쉬우며, 다양한 데이터 형태를 처리할 수 있습니다.
단점: 과적합(overfitting)되기 쉬움.
# 2-2 랜덤 포레스트(Random Forest)
랜덤 포레스트(Random Forest)
정의: 여러 개의 결정 트리를 결합한 앙상블 학습 방법입니다.
장점:
단일 결정 트리에 비해 과적합을 방지합니다.
분산과 편향을 줄이며 일반화 성능을 높입니다.
작동 원리:
데이터 샘플링: 훈련 데이터의 랜덤 서브셋 생성(부트스트랩 샘플링).
랜덤성 도입: 각 트리가 서로 다른 특성 집합을 사용.
다수결(Majority Voting): 모든 트리의 예측 결과를 결합해 최종 예측.
# 교차 검증 및 그리드 탐색
교차 검증(Cross Validation): 데이터를 여러 번 나눠 모델의 일반화 성능을 평가.
그리드 탐색(Grid Search): 여러 하이퍼파라미터 조합을 시도해 최적값을 찾음.
# 3. GridSearchCV
사용자가 직접 모델의 하이퍼 파라미터의 값을 리스트로 입력하면 값에 대한 경우의 수마다 예측 성능을 측정 평가하여 비교하면서 최적의 하이퍼 파라미터 값을 찾는 과정을 진행