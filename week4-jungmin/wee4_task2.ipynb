{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 인공 신경망\n",
        "\n",
        "> ANN, Artificial Neural Networks\n",
        "\n",
        "## 들어가기 전에\n",
        "\n",
        "다음 참고 자료를 이용하여 추가 학습하였습니다."
      ],
      "metadata": {
        "id": "00rHSoZXkOZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron\n",
        "\n",
        "1957년 프랭크 로젠블라트가 제안한 가장 기본적인 인공 신경망 모델. 입력 값에 가중치를 곱하고 더한 값을 기준으로 출력을 만들고 결과를 결정하는 계단 함수를 사용한다.\n",
        "\n",
        "선형 문제만 해결할 수 있고, XOR 등 비선형 문제는 해결할 수 없다는 문제가 있다.\n",
        "\n",
        "## 다층 퍼셉트론\n",
        "\n",
        "입력층, 은닉층, 출력층으로 구성. 순방향 신경방의 일종으로, 여러 개의 계층을 갖는다.\n",
        "\n",
        "- 은닉층: 데이터 처리 및 특징 추출\n",
        "\n",
        "여러 은닉층으로 복잡한 비선형 문제도 해결할 수 있다. 퍼셉트론과 달리 비선형 활성화 함수를 통해 더 복잡한 패턴 학습이 가능하다.\n",
        "\n",
        "## 역전파 알고리즘\n",
        "\n",
        "신경망 훈련을 위한 핵심 알고리즘으로 신경망의 출력 오차를 기반으로 가중치를 조정하여 학습 진행.\n",
        "\n",
        "1. 순전파\n",
        "2. 오차 계산\n",
        "3. 역전파\n",
        "4. 경사하강법\n",
        "\n",
        "순서대로 진행하며, 큰 신경망을 효율적으로 학습할 수 있는 딥러닝의 핵심 기법이다.\n",
        "\n",
        "## 신경망 설계의 주요 요소\n",
        "\n",
        "은닉층이 하나라도 충분한 뉴런 수를 가지면 이론상 복잡한 함수도 근사 가능하지만, 심층 신경망은 파라미터 효율성이 높아 더 적은 뉴런으로도 복잡한 문제 해결이 가능하다.\n",
        "\n",
        "한편, 뉴런 수는 첫 번째 은닉 층에 많은 뉴런을 두고 점차 줄이는 구조가 효과적인 것으로 알려져 있으며 과대 적합을 방지하기 위해 규제 기법을 사용한다.\n",
        "\n",
        "### 학습률 (Learning Rate)\n",
        "\n",
        "가장 중요한 하이퍼 파라미터로 학습률이 너무 크면 발산, 너무 작으면 학습 속도가 느려지는 경향이 있다.\n",
        "\n",
        "그래서 최적의 학습률은 손실 그래프에서 손실이 급격히 감소하기 시작하는 지점의 약 10배 낮은 값이다.\n",
        "\n",
        "### 배치 크기\n",
        "\n",
        "작으면 더 좋은 일반화 성능을, 크면 훈련 속도 향상을 보인다.\n",
        "\n",
        "## 하이퍼 파라미터 튜닝\n",
        "\n",
        "신경망 성능은 층 개수, 뉴런 수, 학습률, 배치 크기 등 하이퍼 파라미터 설정에 따라 달라지는데,랜덤 서치, 그리드 서치, 케라스 튜너 등의 전략이 있다.\n",
        "\n",
        "### 랜덤 서치\n",
        "\n",
        "파라미터 값의 분포 혹은 범위에 들어올 때 학습 잘되는 여부에 대한 경험을 토대로 해당 구간에서 더 많이 탐색하고 해당 파라미터에 대해 결과가 어떻게 되는지 본 후 바꿔서 학습해 볼 수 있다.\n",
        "\n",
        "### 그리드 서치\n",
        "\n",
        "가능한 모든 조합들, 파라미터별 구간 정해서 등간격으로 값 샘플링한다.\n"
      ],
      "metadata": {
        "id": "9Z5YOp4sxcNx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hKiZDLIecEq",
        "outputId": "865ebb3f-4262-4286-929e-f1e558bf15d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8774 - loss: 0.4342\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9642 - loss: 0.1200\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0772\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9825 - loss: 0.0568\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.0450\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "테스트 정확도 97.8100 퍼센트\n",
            "\n",
            "테스트 샘플 예측 값 6.537991e-08\n",
            "테스트 샘플 실제 값 7\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Dense(units=128, input_shape=(784, ), activation=\"relu\"),\n",
        "        Dense(units=10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# MNIST 데이터셋을 로드하고 전처리합니다.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# TODO: 데이터 형태를 (784,)로 변환하고, 0~1 범위로 정규화하세요.\n",
        "# 1차원 텐서로 변환\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "# 정수를 실수형으로, 0~1 범위로 정규화\n",
        "x_train = x_train.astype(np.float32)/255.0\n",
        "x_test = x_test.astype(np.float32)/255.0\n",
        "# 원핫코드 변환 (categorical_crossentropy 사용 위함)\n",
        "y_train_o = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_o = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# TODO: Adam 옵티마이저, sparse categorical crossentropy 손실 함수, accuracy 메트릭으로 모델을 컴파일하세요.\n",
        "model = create_model()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# TODO: 배치 크기를 64로, 에포크 수를 5로 설정하여 모델을 훈련하세요.\n",
        "#\n",
        "model.fit(x_train, y_train_o, batch_size=32, epochs=5)\n",
        "\n",
        "# TODO: 모델을 평가하고 테스트 정확도를 출력하세요.\n",
        "#\n",
        "ev = model.evaluate(x_test, y_test_o, verbose=0)\n",
        "res = model.predict(x_test)\n",
        "print(\"테스트 정확도 %.4f 퍼센트\\n\" % (ev[1]*100))\n",
        "print(\"테스트 샘플 예측 값\", res[0][0])\n",
        "print(\"테스트 샘플 실제 값\", y_test[0])"
      ]
    }
  ]
}