# 심층 신경망 훈련

> Deep Neural Networks(DNN)

심층 신경망에선 모델이 복잡해질수록 은닉층의 개수가 많은 신경망 모델을 학습시켜야 하지만, 은닉층의 깊이가 깊어질수록 발생하는 문제에도 신경써야 한다.

- 그래디언트 소실(vanishing gradient), 폭주(exploding)
- 학습 시간 증가
- 과적합(overfitting) 가능성 증가

## 읽기 전에

다음 추가 참고 자료를 이용해 공부하였습니다.

- [오차역전파 개념](https://sacko.tistory.com/19)
- 2024-1 DSC공유대학 딥러닝 (충남대학교) 5주차

## 오차역전파 (back-propagation)

순전파(input to output) 결과가 만족스럽지 않을 때, 본래 순서에서 역순으로 오차를 다시 보내며 가중치를 다시 갱신한다.

결과에 영향을 많이 미친 노드에 더 많은 오차를 돌려주게 된다.

## 문제점들

### 그래디언트 소실

오차역전파에서 각 가중치 매개변수를 갱신할 때, 그래디언트(gradient)가 점점 작아져 결국 갱신되지 않는 경우가 발생한다. 이를 '그래디언트 소실'이라고 한다. (vanishing gradient)

### 그래디언트 폭주

소실과 반대로 입력층으로 갈수록 가중치 매개변수가 기하급수적으로 커지는 경우를 그래디언트 폭주(exploding gradient)라고 부르며, 가중치가 발산(diverse)하게 되어 학습이 제대로 이뤄지지 않는다.

## 활성화 함수(활성 함수)

> activation function

입력 신호의 총합을 출력 신호로 변환하는 함수. 이전 계층에 있는 노드, 인공 뉴런에서 다음 뉴런으로 출력하는 과정에서 사용한다.

주로 선형 변수와 함께 쓰이는 비선형 함수로, 시그모이드, 하이퍼볼릭 탄젠트, ReLU, 리키 ReLU, 맥스아웃, ELU 등의 함수가 있다.

### 시그모이드 함수

대표적인 활성 함수로, 입력 신호의 총합을 0~1 사이의 값으로 바꿔준다. 값이 커질수록 뉴런의 활성화률(firing rate)은 1, 작아질수록 0으로 수렴한다.

값이 '부드럽게' 분류되지만, 미분값이 0에 가까워 의미가 없어지는 구간이 존재한다. 즉, 값이 변할 때 증가폭이 큰 구간에선 적절하지만 충분히 크거나 작은 양 끝 부분(0, 1)에서는 별 의미가 없다. (**그래디언트 포화** 혹은 소멸)

또한, 원점 중심이 아니라서 평균이 0이 아닌 0.5로, 항상 양수를 출력하기 때문에 출력의 가중치 합이 입력의 가중치 합보다 커질 가능성이 높다. 이를 **편향 이동(bias shift)**이라고 하며, 이 이유로 각 레이어를 지날 때마다 분산이 계속 커져 높은 레이어에서는 활성 함수의 출력이 0이나 1로 수렴, 그래디언트 소실 문제가 일어난다.

### 하이퍼볼릭 탄젠트 함수

> hyperbolic tangent

시그모이드 함수의 대체제로 사용할 수 있는 활성 함수. 원점 중심이므로 편향 이동이 나타나지 않는다는 점에서 시그모이드 대비 개선된 측면이 있다.

그러나 그래디언트 포화 문제에서 자유롭지 않다.

### ReLU

> Rectified Linear Unit

연속 함수를 사용했기 때문에 그래디언트 포화 문제에서 벗어나지 못했다는 착안점에서 고안. 입력이 0 이상이면 값을 그대로, 0보다 작으면 0을 출력한다. 따라서 0 이상 구간에서는 수렴하지 않으며, 단순 출력이므로 계산 속도가 빠르다.

그러나 가중치 초기화를 잘못하거나 학습률(learning rate)이 너무 크면 뉴런이 0만 출력하는 문제가 발생할 수 있다. 함수 구조 상의 문제.

### Leaky ReLU

0보다 클 때는 값을 그대로 가지지만, 0 미만에서 하나의 선형 함수를 추가로 부여하고 아주 작은 기울기를 둔다. 0보다 작은 구간이 0만 갖는 문제를 해결하자는 목표를 위해 고안한 방법이다.

### ELU(Exponential Linear Unit)

0보다 클 때는 값을 그대로 갖지만, 하나의 선형 함수를 추가로 주고 아주 작은 기울기로 부드럽게 만든다.

### 맥스아웃

활성함수도 학습으로 찾아내자는 선형 함수들의 최댓값 함수. 선형 함수들의 조합을 이용해 구하는 방식으로, 일반화된 형태이지만 좀 더 복잡하고 추가적인 학습이 필요하다.

## 가중치 초기화

> Weight Initialization

가중치 초기 값을 어떻게 초기화하느냐에 따라 학습 성능이 달라진다.

### 초기 값이 0이거나 같으면

모든 뉴런이 같은 출력값을 내보내고, 역전파 시 각 뉴런이 같은 그래디언트 값을 갖는다. 그런데 학습이 잘 되려면 *각 뉴런이 가중치에 따라 비대칭*이 되는 것이 좋다. 비대칭이 아니면 아무리 많은 뉴런이라도 하나인 것처럼 작동하기에 학습이 제대로 되지 않는다. 그래서 이렇게 하면 **안 된다.**

### 작은 난수(Small Random Number)이면

활성 함수가 sigmoid일 때, 가중치 초기값을 큰 값으로 하면 0과 1로 수렴하면서 그래디언트 소실이 발생한다. ReLU라면 절대값이 클 경우 음수면 dead ReLU 문제가 발생하고 양수면 그래디언트 폭주가 발생한다. 즉, *큰 값으로는 초기화하지 않아야 한다.*

그래서 작게 하되 같은 값을 갖지는 않도록 해야 한다. 일반적으로 가중치 초기 값은 *평균이 0, 표준 편차가 0.01*인 정규 분포(가우시안 분포)를 따른 값으로 초기화된다. (임의 값)

그러나 얕은 신경망에서는 제대로 작동하더라도 깊어지면 문제가 발생한다. 이렇게 초기화하고 tanh 활성 함수를 사용하면 첫 번째 은닉층을 제외한 나머지 층이 모두 0을 출력하는 문제가 발생할 수 있다. 그러면 모든 뉴런의 그래디언트 값이 같아져서 학습이 제대로 작동하지 않는다.

그렇다면 *평균이 0, 표준 편차가 1*인 정규 분포를 따르면 어떨까? 이때는 tanh의 출력이 -1과 1로 집중되어 그래디언트 소실이 발생한다.

**따라서, 작은 난수 방법도 심층 신경망에는 적절치 않다.**

결론적으로 제대로된 학습을 위해서는 각 뉴런의 활성 함수 출력 값이 고르게 분포되어야 하며, 층과 층 사이 다양한 데이터가 흘러야(순방향, 역방향 모두) 신경망 학습이 효율적이다.

### Xavier Intialization

Xavier Glorot과 Yoshua Bengio가 제시한 초기화 방법. 적절한 데이터가 흐르기 위해서는 각 층 출력에 대한 분산이 입력 분산과 같아야 하고, 역전파에서 층 통과 전 후의 그래디언트 분산이 같아야 한다고 주장하였다.

이 초기화 방법에서는 활성 함수가 선형이라고 가정한다. sigmoid 계열 함수는 좌우 대칭이며 가운데가 선형인 것으로 볼 수 있다.

방법 적용 시 tanh 활성화 함수에서는 작은 난수 초기화 방법보다 넓은 분포를 보인다.

Tensorflow에서 사용 시 `tf.contrib.xavier_initializer`를 제공한다.

### He Initialization

Xavier Intialization이 ReLU에서 층이 깊어질수록 0으로 치우치는 문제를 해결하기 위한 방법. Kaiming He가 제안.

Xavier 초기값의 제곱근 2배 하였고, 이는 ReLU에서 입력이 음수일 때 출력이 모두 0이라서 더 넓게 분포시키기 위함이다.

Tensorflow에서 이용 시 `tf.keras.initializers.he_normal` 혹은 `tf.keras.initializers.he_uniform`을 이용할 수 있다.

## 배치 정규화

> BN, Batch Normalization

심층 신경망 학습에서는 ReLU와 He 초기화 시 학습 초기 단계에서 그래디언트 소실, 폭주 문제를 줄일 수 있었지만, 학습하는 동안 같은 문제가 또 발생할 수 있다.

2015년 Sergety Loffe, Christian Szegedy가 제시한 방법이 바로 배치 정규화로, 각 층의 활성 함수 출력 값 분포가 고루 분포되도록 강제하여 각 층에서의 활성 함수 출력 값이 정규 분포를 이루도록 하는 방법이다.

즉, 학습하는 동안 이전 층에서의 가중치 매개변수 변화로 활성 함수 출력 값 분포가 변화하는 내부 공변량 변화 문제를 줄이는 방법이라고 할 수 있다.

배치 정규화는 미니 배치 단위로 데이터의 분포가 평균 0, 분산 1이 되도록 정규화한다. 단, 입력 데이터에 대해 정규화하면 대부분 0에 가깝게 되므로 sigmoid 함수에서 선형(linearity) 구간에 빠진다. 이 문제를 위해 scaling, shifting을 적용(이 값에 대한 가중치, 편향)한다.

scaling과 shifting은 각각 1, 0으로 시작해 역전파 과정으로 적합한 값이 되도록 조정한다.

tanh, sigmoid 같은 활성화 함수에 대해 그래디언트 소실 문제가 감소하고, 가중치 초기화에 덜 민감하며, 학습률을 크게 잡아도 그래디언트가 잘 수렴하고 과적합을 억제한다.

단, 그럼에도 불구하고 규제 효과가 크지 않기 때문에 과적합 방지를 위해 Dropout을 함께 쓰는 것이 낫다.

Tensorflow에서는 `tf.nn.batch_normalization()`과 `tf.layers.batch_normalization()` 두 가지를 제공하는데, 후자는 평균과 표준 편차를 알아서 계산해준다.

### 미니 배치 경사 하강법

전체 훈련 데이터를 쓰는 것도, 각각 쓰는 것도 아닌 데이터를 작은 단위로 나눠 훈련하는 방식. 전체 데이터가 몇 가지로 쪼개지므로 각각 병렬적인 학습 후 취합이 가능하고, SGD 대비 지역적 최적값으로 빠질 위험이 적다.

다만, 데이터마다 적절한 배치 크기에 대한 시도가 필요하다.

### 테스트 단계에서의 BN

테스트, 추론 단계에서는 평균과 표준 편차를 계산할 미니 배치가 없어 전체 세트의 평균과 표준 편차를 활용한다. 단, 전체 양이 너무 많기 때문에, 각 n개의 미니 배치에 대한 평균과 표준 편차를 이용해 전체 값을 대신한다.

혹은, 모델 학습 단계에서 지수 감수 이동 평균법으로 평균과 표준 편차를 계산할 수 있다.