Week4 개념 정리
===
심층 신경망 훈련
---
심층 신경망에선 모델이 복잡해질수록 hidden layer의 개수가 많은 신경망 모델을 학습시켜야 한다. 하지만 이러한 깊은 모델을 학습시키는데에는 다음과 같은 문제가 발생할 확률이 높다.

- 그래디언트 소실(vanishing gradient) 또는 폭주(exploding)가 발생할 수 있다.
- 모델이 복잡하고 커질수록 학습시간이 매우 느려진다.
- 모델이 복잡할수록 오버피팅(overfitting)될 위험이 크다.

그래디언트 소실 : 그래디언트가 점점 작아져 결국 가중치 매개변수가 업데이트 되지 않는 경우
그래디언트 폭주 : 반대로 역전파에서 그래디언트가 점점 커져 입력층으로 갈수록 가중치 매개변수가 기하급수적으로 커지게 되는 경우
해결방법 : 활성함수로 시그모이드 함수나 tanh 함수 대신 ReLU 함수를 사용한다. 

배치 정규화
---
배치 정규화는 각 층의 활성화 함수의 출력값 분포가 골고루 분포되도록 '강제'하는 방법으로, 각 층에서의 활성화 함수 출력값이 정규분포(normal distribution)를 이루도록 하는 방법이다.
즉 학습하는 동안 이전 레이어에서의 가중치 매개변수가 변함에 따라 활성화 함수 출력값의 분포가 변화하는 내부 공변량 변화(Internal Covariate Shift) 문제를 줄이는 방법이 바로 배치 정규화 기법이다.

- tanh나 sigmoid 같은 활성화 함수에 대해 그래디언트 소실(vanishing gradient)문제가 감소한다.
- 가중치 초기화에 덜 민감하다. 가중치 초기값에 크게 의존하지 않기 때문에 가중치 초기화 기법에 대해 크게 신경 쓰지 않아도 된다.
- 학습률(learning rate)를 크게 잡아도 gradient descent가 잘 수렴한다.
- 오버피팅을 억제한다. BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소한다. 하지만, BN로 인한 규제는 효과가 크지 않기 때문에 드롭아웃을 함께 사용하는 것이 좋다.

퍼셉트론과 다층 퍼셉트론
---
퍼셉트론 : 가장 기본적인 인공 신경망 모델
- 선형 문제만 해결 가능.
- XOR과 같은 비선형 문제를 해결할 수 없음.

다층 퍼셉트론 : 퍼셉트론에 여러 은닉층이 추가된 형태
- 여러 은닉층으로 복잡한 비선형 문제도 해결 가능.
- 퍼셉트론과 달리 비선형 활성화 함수(예: ReLU, 시그모이드)를 사용하여 더 복잡한 패턴 학습 가능.

역전파 알고리즘
---
신경망 훈련을 위한 핵심 알고리즘. 신경망의 출력 오차를 기반으로 가중치를 조정하여 학습을 진행.

1. 순전파 (Forward Propagation): 입력 데이터가 입력층 → 은닉층 → 출력층으로 전달되며 결과를 계산.
2. 오차 계산: 예측값과 실제값의 차이를 손실 함수로 측정.
3. 역전파 (Backward Propagation): 오차를 출력층에서 입력층으로 전파하며, 각 가중치가 오차에 얼마나 기여했는지 계산.
4. 경사 하강법 (Gradient Descent): 가중치를 조정하여 오차를 최소화.

- 큰 신경망을 효율적으로 학습 가능.
- 딥러닝 기술의 핵심 기법.