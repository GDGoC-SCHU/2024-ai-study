# 그래디언트 클리핑

> Gradient Clipping

그래디언트 폭주(exploding gradient) 문제를 줄이기 위한 방법으로, 역전파 단계에서 그래디언트 값이 특정 임계값(threshold)을 넘지 않도록 잘라낸다.

# 심층 신경망 재사용

규모가 매우 큰 DNN 모델 학습 시 새로 학습하는 작업은 느리다. 이때 기존 학습된 모델이 존재하면 하위 층(lower layer)을 가져와 재사용하는 것이 학습 속도를 빠르게 하고 학습에 필요한 데이터 세트도 적다. 이러한 방법을 Transfer Learning이라고 한다.

이때 재사용에 사용하지 않을 층은 동결(freezing)하고, 사용할 층은 학습 가능한 상태로 유지한다. 동결된 층은 학습 과정에서 변하지 않으므로 가장 마지막에 동결된 층 출력을 캐싱하여 활용하면 학습 속도가 개선된다.

# 참고 자료

## Task 1

- 2024-2 순천향대학교 컴퓨터소프트웨어공학과 "머신러닝" (강미선 교수님)
- 