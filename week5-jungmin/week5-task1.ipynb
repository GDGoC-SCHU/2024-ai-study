{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "## 그래디언트 클리핑\n",
        "\n",
        "> Gradient Clipping\n",
        "\n",
        "그래디언트 폭주(exploding gradient) 문제를 줄이기 위한 방법으로, 역전파 단계에서 그래디언트 값이 특정 임계값(threshold)을 넘지 않도록 잘라낸다.\n",
        "\n",
        "## 심층 신경망 재사용\n",
        "\n",
        "규모가 매우 큰 DNN 모델 학습 시 새로 학습하는 작업은 느리다. 이때 기존 학습된 모델이 존재하면 하위 층(lower layer)을 가져와 재사용하는 것이 학습 속도를 빠르게 하고 학습에 필요한 데이터 세트도 적다. 이러한 방법을 Transfer Learning이라고 한다.\n",
        "\n",
        "이때 재사용에 사용하지 않을 층은 동결(freezing)하고, 사용할 층은 학습 가능한 상태로 유지한다. 동결된 층은 학습 과정에서 변하지 않으므로 가장 마지막에 동결된 층 출력을 캐싱하여 활용하면 학습 속도가 개선된다."
      ],
      "metadata": {
        "id": "d-Pq95J_yoJ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqgsySqoyMj4",
        "outputId": "039c4dbf-9a50-4dd2-e386-84f3fec8bab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8347 - loss: 0.5456\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.1316\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9747 - loss: 0.0832\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9815 - loss: 0.0627\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9854 - loss: 0.0470\n"
          ]
        }
      ],
      "source": [
        "# 기존 모델 생성 및 저장\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# 기존 모델 생성\n",
        "# 입력 층에서 지난 PR 피드백을 반영하여 Input() 함수를 사용하도록 변경하였습니다.\n",
        "def create_base_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(784,)),\n",
        "        Dense(128, activation='relu', name='hidden1'),\n",
        "        Dense(64, activation='relu', name='hidden2'),\n",
        "        Dense(32, activation='relu', name='hidden3'),\n",
        "        Dense(10, activation='softmax', name='output'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# MNIST 데이터 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# 모델 생성 및 컴파일\n",
        "base_model = create_base_model()\n",
        "base_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "base_model.fit(x_train, y_train, batch_size=64, epochs=5)\n",
        "\n",
        "# 모델 저장\n",
        "base_model.save('./my_model.keras')\n",
        "# 'my_model.ckpt'는 Keras에서 'h5' 형식으로 저장됩니다.\n",
        "# 오류 메시지에 따르면 `.keras` 포맷 사용을 권장하여 변경하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# 저장된 기존 모델('my_model.keras') 호출\n",
        "base_model = load_model(\"./my_model.keras\")\n",
        "\n",
        "# 새로운 입력 레이어를 정의 (입력 형태: (784,))\n",
        "input_layer = Input(shape=(784,))\n",
        "\n",
        "# 기존 모델에서 hidden1부터 hidden3까지를 가져와 연결\n",
        "hidden1 = base_model.get_layer(\"hidden1\")\n",
        "hidden2 = base_model.get_layer(\"hidden2\")\n",
        "hidden3 = base_model.get_layer(\"hidden3\")\n",
        "\n",
        "# hidden1과 hidden2 레이어 동결\n",
        "for layer in [\"hidden1\", \"hidden2\"]:\n",
        "  base_model.get_layer(layer).trainable = False\n",
        "\n",
        "# 새로운 hidden4와 logits 레이어 추가\n",
        "hidden4 = Dense(16, activation=\"relu\", name=\"hidden4\")\n",
        "logits = Dense(10, activation=\"softmax\", name=\"logits\")\n",
        "\n",
        "# 새로운 Transfer Learning 모델 생성\n",
        "transfer_model = Sequential([\n",
        "    input_layer,\n",
        "    hidden1,\n",
        "    hidden2,\n",
        "    hidden3,\n",
        "    hidden4,\n",
        "    logits\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "transfer_model.compile(optimizer=Adam(learning_rate=0.001), loss=SparseCategoricalCrossentropy, metrics=['accuracy'])\n",
        "\n",
        "# MNIST 데이터셋 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "# 모델 학습\n",
        "history = transfer_model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)\n",
        "\n",
        "# 모델 평가 및 테스트 정확도 출력\n",
        "test_loss, test_acc = transfer_model.evaluate(x_test, y_test)\n",
        "print(f\"테스트 정확도: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AurDXL4p602g",
        "outputId": "beadd6b7-2b11-4e0f-fc64-17a981cd6707"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6893 - loss: 130.7119 - val_accuracy: 0.9199 - val_loss: 2.3506\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9232 - loss: 1.8374 - val_accuracy: 0.8935 - val_loss: 1.0221\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8920 - loss: 0.9105 - val_accuracy: 0.9266 - val_loss: 0.6167\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9236 - loss: 0.5125 - val_accuracy: 0.9310 - val_loss: 0.4144\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9283 - loss: 0.3715 - val_accuracy: 0.9365 - val_loss: 0.3699\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9196 - loss: 0.9573\n",
            "테스트 정확도: 0.9310\n"
          ]
        }
      ]
    }
  ]
}