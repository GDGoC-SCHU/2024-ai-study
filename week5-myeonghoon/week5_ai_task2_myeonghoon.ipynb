{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "과제: 사용자 정의 층과 손실 함수를 사용한 신경망 만들기\n",
        "목표\n",
        "텐서플로와 케라스를 사용하여 사용자 정의 층과 손실 함수를 활용한 신경망을 설계하고 훈련합니다.\n",
        "\n",
        "과제 요구사항\n",
        "1. 데이터 준비\n",
        "\n",
        "데이터셋: sklearn.datasets에서 제공하는 make_regression 데이터를 사용합니다.\n",
        "데이터 처리:\n",
        "훈련 데이터(80%) 와 테스트 데이터(20%) 로 분리하세요.\n",
        "입력 데이터를 표준화하여 평균 0, 표준편차 1로 변환하세요.\n",
        "2. 사용자 정의 층\n",
        "\n",
        "MyDenseLayer라는 사용자 정의 층을 만드세요.\n",
        "요구사항:\n",
        "입력 크기에 따라 가중치(weight) 와 편향(bias) 를 생성하고 학습 가능하도록 설정합니다.\n",
        "입력 데이터를 받아 ReLU 활성화 함수를 적용합니다.\n",
        "3. 사용자 정의 손실 함수\n",
        "\n",
        "후버 손실(Huber Loss) 를 구현하세요.\n",
        "요구사항:\n",
        "임계값(delta)은 1로 설정합니다.\n",
        "작은 오차는 제곱 손실, 큰 오차는 선형 손실로 처리합니다.\n",
        "4. 모델 설계 및 훈련\n",
        "\n",
        "사용자 정의 층과 후버 손실 함수를 사용하여 신경망을 설계합니다.\n",
        "요구사항:\n",
        "구조: 2개의 은닉층(각 32개의 뉴런)과 1개의 출력층.\n",
        "Optimizer: Adam.\n",
        "평가지표: MSE (Mean Squared Error).\n",
        "훈련: 10 epoch, batch size=32.\n",
        "5. 평가 및 예측\n",
        "\n",
        "요구사항:\n",
        "테스트 데이터에서 MSE를 출력하세요.\n",
        "테스트 데이터 중 첫 번째 샘플의 예측값과 실제값을 출력하세요.\n",
        "출력\n",
        "모델 훈련 중 손실값 출력:\n",
        "테스트 데이터에서의 MSE 출력:\n",
        "첫 번째 샘플의 예측값과 실제값 출력:"
      ],
      "metadata": {
        "id": "QiaJaKxrCQR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o97v3kTlB8pv",
        "outputId": "76fa3b32-a399-4ab7-b70d-c9164d788310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.4422 - mse: 1.0666 - val_loss: 0.3399 - val_mse: 0.7759\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3797 - mse: 0.8685 - val_loss: 0.2688 - val_mse: 0.5891\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2659 - mse: 0.5829 - val_loss: 0.1181 - val_mse: 0.2400\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0962 - mse: 0.2014 - val_loss: 0.0164 - val_mse: 0.0327\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0164 - mse: 0.0329 - val_loss: 0.0089 - val_mse: 0.0177\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0072 - mse: 0.0143 - val_loss: 0.0056 - val_mse: 0.0111\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0071 - mse: 0.0143 - val_loss: 0.0030 - val_mse: 0.0059\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - mse: 0.0074 - val_loss: 0.0024 - val_mse: 0.0048\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0028 - mse: 0.0055 - val_loss: 0.0021 - val_mse: 0.0042\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0020 - mse: 0.0041 - val_loss: 0.0019 - val_mse: 0.0037\n",
            "Epoch 1: Loss = 0.42548996210098267\n",
            "Epoch 2: Loss = 0.374498188495636\n",
            "Epoch 3: Loss = 0.24352571368217468\n",
            "Epoch 4: Loss = 0.06527294218540192\n",
            "Epoch 5: Loss = 0.014599730260670185\n",
            "Epoch 6: Loss = 0.007749731652438641\n",
            "Epoch 7: Loss = 0.005060367286205292\n",
            "Epoch 8: Loss = 0.00330030987970531\n",
            "Epoch 9: Loss = 0.002631081733852625\n",
            "Epoch 10: Loss = 0.0022264798171818256\n",
            "Test MSE: 0.004172916989773512\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "Predicted value: 48.98503875732422\n",
            "Actual value: 42.67137812993431\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. 데이터 준비\n",
        "# 데이터 생성\n",
        "data, target = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "# 타겟 차원 변경\n",
        "target = target.reshape(-1, 1)\n",
        "\n",
        "# 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터 표준화\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train = scaler_X.fit_transform(X_train)\n",
        "X_test = scaler_X.transform(X_test)\n",
        "y_train = scaler_y.fit_transform(y_train)\n",
        "y_test = scaler_y.transform(y_test)\n",
        "\n",
        "# 2. 사용자 정의 층\n",
        "class MyDenseLayer(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(MyDenseLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.weight = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "            name=\"weight\"\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"bias\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.relu(tf.matmul(inputs, self.weight) + self.bias)\n",
        "\n",
        "# 3. 사용자 정의 손실 함수\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    error = tf.abs(y_true - y_pred)\n",
        "    is_small_error = error <= delta\n",
        "    small_error_loss = 0.5 * tf.square(error)\n",
        "    big_error_loss = delta * error - 0.5 * tf.square(delta)\n",
        "    return tf.where(is_small_error, small_error_loss, big_error_loss)\n",
        "\n",
        "# 4. 모델 설계 및 훈련\n",
        "model = Sequential([\n",
        "    MyDenseLayer(32),\n",
        "    MyDenseLayer(32),\n",
        "    tf.keras.layers.Dense(1)  # 출력층\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=lambda y_true, y_pred: huber_loss(y_true, y_pred), metrics=[\"mse\"])\n",
        "\n",
        "# 모델 훈련\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 손실값 출력\n",
        "for epoch, loss in enumerate(history.history['loss'], 1):\n",
        "    print(f\"Epoch {epoch}: Loss = {loss}\")\n",
        "\n",
        "# 5. 평가 및 예측\n",
        "# 테스트 데이터에서의 MSE\n",
        "mse = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "print(f\"Test MSE: {mse}\")\n",
        "\n",
        "# 첫 번째 샘플의 예측값과 실제값\n",
        "sample_pred = model.predict(X_test[:1])\n",
        "actual_value = y_test[:1]\n",
        "\n",
        "# 스케일링 복원\n",
        "sample_pred = scaler_y.inverse_transform(sample_pred)\n",
        "actual_value = scaler_y.inverse_transform(actual_value)\n",
        "\n",
        "print(f\"Predicted value: {sample_pred[0][0]}\")\n",
        "print(f\"Actual value: {actual_value[0][0]}\")"
      ]
    }
  ]
}