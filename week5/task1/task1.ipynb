{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84a73b50",
      "metadata": {
        "id": "84a73b50"
      },
      "source": [
        "# 그래디언트 클리핑(Gradient Clipping)\n",
        "\n",
        "그래디언트 클리핑(Gradient Clipping)은 그래디언트 폭주(exploding gradient) 문제를 줄이는 방법이며, 역전파(backprop) 단계에서 그래디언트 값이 아래의 그림과 같이 특정 임계값(threshold)을 넘지 않도록 잘라내는 방법입니다.\n",
        "\n",
        "![그래디언트 클리핑](https://github.com/GDGoC-SCHU/2024-ai-study/blob/main/week5/task1/png/Gradient%20Clipping.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976928d1",
      "metadata": {
        "id": "976928d1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "80765b0f",
      "metadata": {
        "id": "80765b0f"
      },
      "source": [
        "## 텐서플로에서 그래디언트 클리핑 구현하기\n",
        "\n",
        "텐서플로에서는 `tf.clip_by_value`를 이용해 그래디언트 클리핑을 구현할 수 있습니다. 아래의 예제는 MNIST 데이터셋을 분류하는 간단한 분류기를 구현한 뒤에 그래디언트 클리핑을 적용한 예제입니다. 아래의 전체 코드는 [ExcelsiorCJH](https://github.com/ExcelsiorCJH/Hands-On-ML/blob/master/Chap11-Training_DNN/Chap11_2-Training_DNN.ipynb) GitHub에서 확인할 수 있습니다. `tf.clip_by_value`를 사용하려면, 아래의 코드에서 옵티마이저(`tf.train.GradientDescentOptimizer`)에 사용해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9020543f",
      "metadata": {
        "id": "9020543f"
      },
      "outputs": [],
      "source": [
        "reset_graph()\n",
        "\n",
        "################\n",
        "# layer params #\n",
        "################\n",
        "n_inputs = 28*28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "# input layer\n",
        "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
        "# output layer\n",
        "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "    hidden1 = tf.layers.dense(inputs, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name='logits')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name='loss')\n",
        "\n",
        "################\n",
        "# Hyper-params #\n",
        "################\n",
        "learning_rate = 0.01\n",
        "threshold = 1.0\n",
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    # 그래디언트 계산\n",
        "    grad_and_vars = optimizer.compute_gradients(loss)\n",
        "    # 그래디언트 클리핑\n",
        "    clipped_grads = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "                     for grad, var in grad_and_vars]\n",
        "    # 클리핑 된 그래디언트 적용\n",
        "    train_op = optimizer.apply_gradients(clipped_grads)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad57c084",
      "metadata": {
        "id": "ad57c084",
        "outputId": "1e756a5b-e317-4948-fdca-6bd805f2301a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 000, valid. Acc: 0.9026\n",
            "epoch: 001, valid. Acc: 0.9244\n",
            "epoch: 002, valid. Acc: 0.9362\n",
            "epoch: 003, valid. Acc: 0.9410\n",
            "epoch: 004, valid. Acc: 0.9460\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_x, batch_y in shuffle_batch(train_x, train_y, batch_size):\n",
        "            sess.run(train_op, feed_dict={inputs: batch_x,\n",
        "                                          labels:batch_y})\n",
        "\n",
        "        # validation\n",
        "        accuracy_val = accuracy.eval(feed_dict={inputs: valid_x, labels: valid_y})\n",
        "        print('epoch: {:03d}, valid. Acc: {:.4f}'.format(epoch, accuracy_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MuSHIFNd2Nlp"
      },
      "id": "MuSHIFNd2Nlp"
    },
    {
      "cell_type": "markdown",
      "id": "05551ce9",
      "metadata": {
        "id": "05551ce9"
      },
      "source": [
        "# 심층 신경망 학습 - 학습된 모델 재사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b07c298",
      "metadata": {
        "id": "5b07c298"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9c38d7",
      "metadata": {
        "id": "4b9c38d7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 일관된 출력을 위해 유사난수 초기화\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "sn.set()\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# 한글출력\n",
        "# matplotlib.rc('font', family='AppleGothic')  # MacOS\n",
        "matplotlib.rc('font', family='Malgun Gothic')  # Windows\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb7fe24d",
      "metadata": {
        "id": "fb7fe24d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7797c923",
      "metadata": {
        "id": "7797c923"
      },
      "source": [
        "규모가 매우 큰 DNN 모델을 학습 시킬 때 처음부터 새로 학습 시키는 것은 학습 속도가 느린 문제가 있습니다. 이러한 경우 기존에 학습된 비슷한 DNN모델이 있을 때 이 모델의 하위층(lower layer)을 가져와 재사용하는 것이 학습 속도를 빠르게 할 수 있을 뿐만아니라 학습에 필요한 Training set도 훨씬 적습니다.\n",
        "\n",
        "예를 들어, 아래의 그림처럼 [CIFAR10](http://www.cs.toronto.edu/~kriz/cifar.html) 데이터셋을 분류(비행기, 자동차, 새, 고양이, 사슴, 개, 개구리, 말, 배, 트럭의 10개  클래스)하는 모델 A가 있고, 분류된 CIFAR10 이미지에서 자동차의 종류를 분류하는 모델인 B를 학습시킨다고 할 때 학습된 모델 A에서의 일부분(lower layer)을 재사용하여 모델 B를 학습 시킬 수 있습니다. 이러한 방법을 **Transfer Learning**이라고 합니다.\n",
        "\n",
        "![Transfer Learning](https://github.com/GDGoC-SCHU/2024-ai-study/blob/main/week5/task1/png/Transfer%20Learning.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d91d8324",
      "metadata": {
        "id": "d91d8324"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "294a01a5",
      "metadata": {
        "id": "294a01a5"
      },
      "source": [
        "## 텐서플로 모델 재사용하기\n",
        "\n",
        "텐서플로에서는 사전에 학습된 모델을 복원하여 새로운 모델을 학습시키는 데 사용할 수 있습니다. 텐서플로의 `tf.train.Saver`클래스를 이용해 학습된 모델을 저장하고 복원할 수 있게합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513473df",
      "metadata": {
        "id": "513473df"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c68dba8e",
      "metadata": {
        "id": "c68dba8e"
      },
      "source": [
        "### 학습된 모델 저장하기\n",
        "\n",
        "아래의 예제 코드는, 5개의 hidden layer로 구성된 MNIST 데이터셋을 분류하는 모델입니다. `tf.train.Saver`를 이용해 학습된 모델을 `'my_model.ckpt'`에 저장하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78c9a1d",
      "metadata": {
        "id": "a78c9a1d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# MNIST Load\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Train & TestSet reshape\n",
        "train_x = train_x.astype(np.float32).reshape(-1, 28*28) / 255.\n",
        "train_y = train_y.astype(np.int32)\n",
        "test_x = test_x.astype(np.float32).reshape(-1, 28*28) / 255.\n",
        "test_y = test_y.astype(np.int32)\n",
        "\n",
        "# Split Validation set from Train set\n",
        "valid_x, train_x = train_x[:5000], train_x[5000:]\n",
        "valid_y, train_y = train_y[:5000], train_y[5000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615fc08f",
      "metadata": {
        "id": "615fc08f"
      },
      "outputs": [],
      "source": [
        "def shuffle_batch(inputs, labels, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(inputs))\n",
        "    n_batches = len(inputs) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        batch_x, batch_y = inputs[batch_idx], labels[batch_idx]\n",
        "        yield batch_x, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5eeb087",
      "metadata": {
        "id": "c5eeb087",
        "outputId": "f7f7d606-d20d-43c5-f603-73d358001f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 00, valid. Acc: 0.8768\n",
            "epoch: 01, valid. Acc: 0.9276\n",
            "epoch: 02, valid. Acc: 0.9462\n",
            "epoch: 03, valid. Acc: 0.9544\n",
            "epoch: 04, valid. Acc: 0.9570\n"
          ]
        }
      ],
      "source": [
        "reset_graph()\n",
        "\n",
        "################\n",
        "# layer params #\n",
        "################\n",
        "n_inputs = 28*28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_hidden5 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "# input layer\n",
        "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"inputs\")\n",
        "# output layer\n",
        "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "    hidden1 = tf.layers.dense(inputs, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name='hidden3')\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='hidden4')\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name='hidden5')\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name='logits')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "\n",
        "################\n",
        "# Hyper-params #\n",
        "################\n",
        "learning_rate = 0.01\n",
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "# Saver 정의\n",
        "MODEL_PATH = './model/'\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# 모델을 쉽게 재사용 할 수 있도록\n",
        "# 텐서플로 컬렉션(collection)에 저장\n",
        "train_vars = {'inputs': inputs, 'labels': labels,\n",
        "              'hidden1': hidden1, 'hidden2': hidden2,\n",
        "              'hidden3': hidden3, 'hidden4': hidden4,\n",
        "              'hidden5': hidden5, 'logits': logits}\n",
        "\n",
        "for key, var in train_vars.items():\n",
        "    tf.add_to_collection(key, var)\n",
        "\n",
        "# Train\n",
        "with tf.Session() as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_x, batch_y in shuffle_batch(train_x, train_y, batch_size):\n",
        "            sess.run(train_op, feed_dict={inputs: batch_x,\n",
        "                                          labels: batch_y})\n",
        "\n",
        "        # validation\n",
        "        accuracy_val = accuracy.eval(feed_dict={inputs: valid_x, labels: valid_y})\n",
        "        print('epoch: {:02d}, valid. Acc: {:.4f}'.format(epoch, accuracy_val))\n",
        "\n",
        "    # model save\n",
        "    save_path = saver.save(sess, os.path.join(MODEL_PATH, 'my_model.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1faab23e",
      "metadata": {
        "id": "1faab23e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0515fd8f",
      "metadata": {
        "id": "0515fd8f"
      },
      "source": [
        "### 학습된 모델을 이용해 4번째 레이어만 수정하기\n",
        "\n",
        "이제 '학습된 모델 저장하기' 에서 저장한 `'my_model.ckpt'`을 이용해, 4번째 hidden layer의 노드 수를 20개로 수정한 뒤 새로운 모델을 학습시키는 코드입니다. 아래의 코드는 위의 코드에서 `tf.add_to_collection`에 저장한 `inputs, labels, hidden3 `를 불러온 뒤, `new_hidden4, new_logits`을 추가한 새로운 모델을 학습하여 `my_new_model.ckpt`에 저장하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2e6de9",
      "metadata": {
        "id": "2f2e6de9",
        "outputId": "d3057221-5040-41bc-dbb9-38c6d10618f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/my_model.ckpt\n",
            "epoch: 00, valid. Acc: 0.9548\n",
            "epoch: 01, valid. Acc: 0.9732\n",
            "epoch: 02, valid. Acc: 0.9696\n",
            "epoch: 03, valid. Acc: 0.9746\n",
            "epoch: 04, valid. Acc: 0.9752\n"
          ]
        }
      ],
      "source": [
        "reset_graph()\n",
        "\n",
        "#################\n",
        "# layers params #\n",
        "#################\n",
        "n_hidden4 = 20  # new hidden\n",
        "n_outputs = 10  # new output\n",
        "\n",
        "MODEL_PATH = './model/'\n",
        "saver = tf.train.import_meta_graph(os.path.join(MODEL_PATH, 'my_model.ckpt.meta'))\n",
        "\n",
        "inputs = tf.get_default_graph().get_collection('inputs')[0]\n",
        "labels = tf.get_default_graph().get_collection('labels')[0]\n",
        "\n",
        "hidden3 = tf.get_default_graph().get_collection('hidden3')[0]\n",
        "\n",
        "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='new_hidden4')\n",
        "new_logits = tf.layers.dense(new_hidden4, n_outputs, name='new_logits')\n",
        "\n",
        "with tf.name_scope('new_loss'):\n",
        "    cross_entropy = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=new_logits))\n",
        "\n",
        "################\n",
        "# Hyper-params #\n",
        "################\n",
        "learning_rate = 0.001\n",
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "\n",
        "with tf.name_scope('new_train'):\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
        "\n",
        "with tf.name_scope('new_eval'):\n",
        "    correct = tf.nn.in_top_k(new_logits, labels, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "# New Saver\n",
        "new_saver = tf.train.Saver()\n",
        "\n",
        "# Train the New Model\n",
        "with tf.Session() as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "    saver.restore(sess, os.path.join(MODEL_PATH, 'my_model.ckpt'))\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_x, batch_y in shuffle_batch(train_x, train_y, batch_size):\n",
        "            sess.run(train_op, feed_dict={inputs: batch_x,\n",
        "                                          labels: batch_y})\n",
        "\n",
        "        # validation\n",
        "        accuracy_val = accuracy.eval(feed_dict={inputs: valid_x, labels: valid_y})\n",
        "        print('epoch: {:02d}, valid. Acc: {:.4f}'.format(epoch, accuracy_val))\n",
        "\n",
        "    # save the new model\n",
        "    save_path = new_saver.save(sess, os.path.join(MODEL_PATH, 'my_new_model.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8362f2",
      "metadata": {
        "id": "4e8362f2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6eca3e8e",
      "metadata": {
        "id": "6eca3e8e"
      },
      "source": [
        "# 텐서플로를 이용한 Transfer Learning\n",
        "\n",
        "TensorFlow를 활용하여 Transfer Learning을 구현합니다. 사전에 학습된 모델 `my_model.ckpt`를 불러와 `hidden1과 hidden2` 레이어는 동결(Freezing)하여 그대로 사용하고, `hidden3` 레이어는 학습 가능한 상태로 유지합니다. 추가적으로, 새로운 `hidden4` 레이어와 `logits` 레이어를 정의하여 Transfer Learning을 수행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9aa653",
      "metadata": {
        "id": "aa9aa653"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7cca8939",
      "metadata": {
        "id": "7cca8939"
      },
      "source": [
        "### 재사용할 레이어 동결(Freezing)하는 방법 (1)\n",
        "\n",
        "먼저, 학습시킬 레이어(`hidden3, hidden4, logits`)와 동결(학습하지 않을)할 레이어(`hidden1, hidden2`)를 TensorFlow의 `tf.get_collection()`을 활용하여 설정해야 합니다.\n",
        "\n",
        "- **학습시킬 레이어**(`hidden3, hidden4, logits`):  \n",
        "  `tf.get_collection()`의 `scope` 인자에 정규표현식으로 학습 대상 레이어를 `'hidden[34]|logits'`와 같이 지정하면, 텐서의 `name`이 매칭되는 변수들을 찾을 수 있습니다. 이 변수들을 `optimizer.minimize()`의 `var_list` 인자에 전달하여 학습할 수 있습니다.\n",
        "\n",
        "```python\n",
        "# 학습시킬 레이어 설정 예시\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                               scope='hidden[34]|logits')  # 정규표현식\n",
        "train_op = optimizer.minimize(loss, var_list=train_vars)\n",
        "```\n",
        "\n",
        "- **재사용할 레이어**(`hidden1~3`):  \n",
        "  이와 유사하게, `tf.get_collection()`의 `scope` 인자에 정규표현식으로 `'hidden[123]'`을 지정하여 필요한 변수들을 찾습니다. 이후, 해당 변수들을 `tf.train.Saver()`에 전달하여 저장된 모델에서 복원합니다.\n",
        "\n",
        "```python\n",
        "# 재사용할 레이어 불러오기 예시\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]')  # 정규표현식\n",
        "restore_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    restore_saver.restore(sess, './model/my_model.ckpt')\n",
        "```\n",
        "\n",
        "위 내용을 기반으로, 하위층(low layer)을 동결한 상태에서 새로운 레이어를 추가하여 모델을 학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15af1dc6",
      "metadata": {
        "id": "15af1dc6",
        "outputId": "43a6c85a-8f65-490c-9ce1-35294570097d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/my_model.ckpt\n",
            "epoch: 00, valid. Acc: 0.9480\n",
            "epoch: 01, valid. Acc: 0.9516\n",
            "epoch: 02, valid. Acc: 0.9580\n",
            "epoch: 03, valid. Acc: 0.9578\n",
            "epoch: 04, valid. Acc: 0.9584\n"
          ]
        }
      ],
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300  # Reusing\n",
        "n_hidden2 = 50  # Reusing\n",
        "n_hidden3 = 50  # Reusing\n",
        "n_hidden4 = 20  # New\n",
        "n_outputs = 10  # New\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs], name='inputs')\n",
        "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "    hidden1 = tf.layers.dense(inputs, n_hidden1,\n",
        "                              activation=tf.nn.relu, name='hidden1')  # Reusing\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
        "                              activation=tf.nn.relu, name='hidden2')  # Reusing\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3,\n",
        "                              activation=tf.nn.relu, name='hidden3')  # Reusing\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4,\n",
        "                              activation=tf.nn.relu, name='hidden4')  # New\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name='logits')  # new\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "\n",
        "################\n",
        "# Hyper-params #\n",
        "################\n",
        "learning_rate = 0.01\n",
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                   scope='hidden[34]|logits')\n",
        "    train_op = optimizer.minimize(cross_entropy, var_list=train_vars)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "# New Saver 정의\n",
        "MODEL_PATH = './model/'\n",
        "new_saver = tf.train.Saver()\n",
        "\n",
        "# Reusing layer load\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]')\n",
        "restore_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "# Train the New Model\n",
        "with tf.Session() as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "    restore_saver.restore(sess, os.path.join(MODEL_PATH, 'my_model.ckpt'))\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_x, batch_y in shuffle_batch(train_x, train_y, batch_size):\n",
        "            sess.run(train_op, feed_dict={inputs: batch_x,\n",
        "                                          labels: batch_y})\n",
        "\n",
        "        # validation\n",
        "        accuracy_val = accuracy.eval(feed_dict={inputs: valid_x, labels: valid_y})\n",
        "        print('epoch: {:02d}, valid. Acc: {:.4f}'.format(epoch, accuracy_val))\n",
        "\n",
        "    # save the new model\n",
        "    save_path = new_saver.save(sess, os.path.join(MODEL_PATH, 'my_transfer_model.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef2ff01",
      "metadata": {
        "id": "4ef2ff01"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f5cd80ca",
      "metadata": {
        "id": "f5cd80ca"
      },
      "source": [
        "### 재사용할 레이어 동결(Freezing)하는 방법 (2)\n",
        "\n",
        "위에서 설명한 `optimizer.minimize()`의 `var_list` 인자를 활용하여 학습할 레이어를 지정하는 방식 대신, `tf.stop_gradient()`를 사용하여 Transfer Learning을 수행할 수도 있습니다. 아래 예시와 같이, 동결(freezing)하려는 마지막 레이어(예: `hidden2`) 바로 뒤에 `tf.stop_gradient()`를 적용하면 해당 레이어에서의 그래디언트 전파가 멈추게 됩니다.\n",
        "\n",
        "```python\n",
        "# tf.stop_gradient()를 사용한 Transfer Learning\n",
        "hidden2 = tf.layers.dense(hidden1, ...)\n",
        "hidden2_stop = tf.stop_gradient(hidden2)\n",
        "hidden3 = tf.layers.dense(hidden2_stop, ...)\n",
        "# ...\n",
        "```\n",
        "\n",
        "아래는 위에서 설명한 방식을 `tf.stop_gradient()`를 활용하여 작성한 코드입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7243422",
      "metadata": {
        "id": "e7243422",
        "outputId": "51c1ab97-4bb6-4f7e-bdac-5183b9fbfb2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/my_model.ckpt\n",
            "epoch: 00, valid. Acc: 0.9504\n",
            "epoch: 01, valid. Acc: 0.9544\n",
            "epoch: 02, valid. Acc: 0.9554\n",
            "epoch: 03, valid. Acc: 0.9562\n",
            "epoch: 04, valid. Acc: 0.9576\n"
          ]
        }
      ],
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300  # Reusing\n",
        "n_hidden2 = 50  # Reusing\n",
        "n_hidden3 = 50  # Reusing\n",
        "n_hidden4 = 20  # New\n",
        "n_outputs = 10  # New\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs], name='inputs')\n",
        "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "    hidden1 = tf.layers.dense(inputs, n_hidden1,\n",
        "                              activation=tf.nn.relu, name='hidden1')  # Reusing\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
        "                              activation=tf.nn.relu, name='hidden2')  # Reusing\n",
        "    hidden2_stop = tf.stop_gradient(hidden2)  # freezing\n",
        "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3,\n",
        "                              activation=tf.nn.relu, name='hidden3')  # Reusing\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4,\n",
        "                              activation=tf.nn.relu, name='hidden4')  # New\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name='logits')  # new\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "\n",
        "################\n",
        "# Hyper-params #\n",
        "################\n",
        "learning_rate = 0.01\n",
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "# New Saver 정의\n",
        "MODEL_PATH = './model/'\n",
        "new_saver = tf.train.Saver()\n",
        "\n",
        "# Reusing layer load\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]')\n",
        "restore_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "# Train the New Model\n",
        "with tf.Session() as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "    restore_saver.restore(sess, os.path.join(MODEL_PATH, 'my_model.ckpt'))\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_x, batch_y in shuffle_batch(train_x, train_y, batch_size):\n",
        "            sess.run(train_op, feed_dict={inputs: batch_x,\n",
        "                                          labels: batch_y})\n",
        "\n",
        "        # validation\n",
        "        accuracy_val = accuracy.eval(feed_dict={inputs: valid_x, labels: valid_y})\n",
        "        print('epoch: {:02d}, valid. Acc: {:.4f}'.format(epoch, accuracy_val))\n",
        "\n",
        "    # save the new model\n",
        "    save_path = new_saver.save(sess, os.path.join(MODEL_PATH, 'my_transfer_model2.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c811c02",
      "metadata": {
        "id": "3c811c02"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f0c098cb",
      "metadata": {
        "id": "f0c098cb"
      },
      "source": [
        "### 동결된 층 캐싱하기\n",
        "\n",
        "위에서 `hidden1`과 `hidden2` 레이어를 재사용하면서 동결(Freezing)했는데, 이처럼 동결된 레이어는 학습 과정에서 변하지 않으므로, 가장 마지막 동결된 레이어(`hidden2`)의 출력을 캐싱(Caching)하여 활용할 수 있습니다. 이는 학습 속도를 개선하는 데 유용합니다.\n",
        "\n",
        "1. **전체 Training Set 처리:**  \n",
        "   Training Set 전체를 한 번 실행하여, 마지막 동결된 레이어(`hidden2`)의 출력을 얻습니다. (충분한 메모리가 있다고 가정합니다.)\n",
        "\n",
        "2. **미니배치 구성:**  \n",
        "   학습하는 동안, Training Set의 원본 데이터 대신 1단계에서 캐싱한 `hidden2` 레이어의 출력으로 미니배치를 구성하여, 다음 레이어에 입력으로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e37993fb",
      "metadata": {
        "id": "e37993fb",
        "outputId": "669e7bb7-3134-4752-d1da-82cfad2a4cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/my_model.ckpt\n",
            "epoch: 00, valid. Acc: 0.9504\n",
            "epoch: 01, valid. Acc: 0.9544\n",
            "epoch: 02, valid. Acc: 0.9554\n",
            "epoch: 03, valid. Acc: 0.9562\n",
            "epoch: 04, valid. Acc: 0.9576\n"
          ]
        }
      ],
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300  # Reusing\n",
        "n_hidden2 = 50  # Reusing\n",
        "n_hidden3 = 50  # Reusing\n",
        "n_hidden4 = 20  # New\n",
        "n_outputs = 10  # New\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs], name='inputs')\n",
        "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "    hidden1 = tf.layers.dense(inputs, n_hidden1,\n",
        "                              activation=tf.nn.relu, name='hidden1')  # Reusing\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
        "                              activation=tf.nn.relu, name='hidden2')  # Reusing\n",
        "    hidden2_stop = tf.stop_gradient(hidden2)  # freezing\n",
        "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3,\n",
        "                              activation=tf.nn.relu, name='hidden3')  # Reusing\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4,\n",
        "                              activation=tf.nn.relu, name='hidden4')  # New\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name='logits')  # new\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    cross_entropy = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "\n",
        "################\n",
        "# Hyper-params #\n",
        "################\n",
        "learning_rate = 0.01\n",
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "# New Saver 정의\n",
        "MODEL_PATH = './model/'\n",
        "new_saver = tf.train.Saver()\n",
        "\n",
        "# Reusing layer load\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]')\n",
        "restore_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "# Train\n",
        "n_batches = len(train_x) // batch_size\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "    restore_saver.restore(sess, os.path.join(MODEL_PATH, 'my_model.ckpt'))\n",
        "\n",
        "    # Caching\n",
        "    h2_cache = sess.run(hidden2, feed_dict={inputs: train_x})\n",
        "    h2_cache_valid = sess.run(hidden2, feed_dict={inputs: valid_x})\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # mini-batch for hidden2\n",
        "        shuffle_idx = np.random.permutation(len(train_x))\n",
        "        hidden2_batches = np.array_split(h2_cache[shuffle_idx], n_batches)\n",
        "        label_batches = np.array_split(train_y[shuffle_idx], n_batches)\n",
        "        for hidden2_batch, label_batch in zip(hidden2_batches, label_batches):\n",
        "            sess.run(train_op, feed_dict={hidden2: hidden2_batch,\n",
        "                                          labels: label_batch})\n",
        "\n",
        "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid,\n",
        "                                                labels: valid_y})\n",
        "        print('epoch: {:02d}, valid. Acc: {:.4f}'.format(epoch, accuracy_val))\n",
        "\n",
        "    # save the new model\n",
        "    save_path = new_saver.save(sess, os.path.join(MODEL_PATH, 'my_caching_model.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1:\n",
        "#### 1. **기존 모델 생성 및 저장**\n",
        "아래 코드를 실행하여 `my_model.h5` 파일을 생성하세요. 이 파일은 학습된 기본 모델로, Transfer Learning 과제에서 활용됩니다.\n",
        "\n",
        "\n",
        "#### 2. **Transfer Learning 과제**\n",
        "이제 학습된 모델(`my_model.h5`)을 기반으로 Transfer Learning을 수행합니다.\n",
        "\n",
        "#### 3. **과제 목표**\n",
        "1. `my_model.h5`에서 `hidden1`과 `hidden2` 레이어를 **동결(Freezing)** 처리합니다.\n",
        "2. 새로운 **hidden4**와 **logits** 레이어를 추가하여 모델을 확장합니다.\n",
        "3. 확장된 모델을 사용하여 **MNIST 데이터셋**을 학습하고, 테스트 데이터로 **성능을 평가**합니다.\n",
        "\n",
        "#### 4. **과제 지시사항**\n",
        "- `my_model.h5` 파일을 불러와 `hidden1`, `hidden2`, `hidden3`를 재사용하고, 새로운 `hidden4`와 `logits`를 추가하세요.\n",
        "- `hidden1`과 `hidden2`는 동결 처리하여 학습되지 않도록 설정합니다.\n",
        "- Transfer Learning 모델을 **Adam 옵티마이저**, **Sparse Categorical Crossentropy** 손실 함수, **Accuracy** 메트릭으로 컴파일하세요.\n",
        "- 모델을 학습한 후 **테스트 정확도**를 출력하세요.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_xoxbvSK9Epb"
      },
      "id": "_xoxbvSK9Epb"
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 모델 생성 및 저장\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 기존 모델 생성\n",
        "def create_base_model():\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(784,), name='hidden1'),\n",
        "        Dense(64, activation='relu', name='hidden2'),\n",
        "        Dense(32, activation='relu', name='hidden3'),\n",
        "        Dense(10, activation='softmax', name='output'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# MNIST 데이터 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# 모델 생성 및 컴파일\n",
        "base_model = create_base_model()\n",
        "base_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "base_model.fit(x_train, y_train, batch_size=64, epochs=5)\n",
        "\n",
        "# 모델 저장\n",
        "base_model.save('./my_model.h5')  # 'my_model.ckpt'는 Keras에서 'h5' 형식으로 저장됩니다.\n"
      ],
      "metadata": {
        "id": "c8QBOEOy9GqW"
      },
      "id": "c8QBOEOy9GqW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# TODO: 저장된 기존 모델('my_model.h5')을 불러오세요.\n",
        "base_model = __________________________\n",
        "\n",
        "# TODO: 새로운 입력 레이어를 정의하세요. (입력 형태: (784,))\n",
        "input_layer = __________________________\n",
        "\n",
        "# TODO: 기존 모델에서 hidden1부터 hidden3까지를 가져와 연결하세요.\n",
        "hidden1 = __________________________\n",
        "hidden2 = __________________________\n",
        "hidden3 = __________________________\n",
        "\n",
        "# TODO: hidden1과 hidden2 레이어를 동결하세요.\n",
        "for layer in __________________________:\n",
        "    __________________________\n",
        "\n",
        "# TODO: 새로운 hidden4와 logits 레이어를 추가하여 모델을 확장하세요.\n",
        "hidden4 = __________________________\n",
        "logits = __________________________\n",
        "\n",
        "# TODO: 새로운 Transfer Learning 모델을 생성하세요.\n",
        "transfer_model = __________________________\n",
        "\n",
        "# TODO: 모델을 컴파일하세요. (옵티마이저: Adam, 손실 함수: Sparse Categorical Crossentropy, 메트릭: Accuracy)\n",
        "transfer_model.compile(\n",
        "    optimizer=__________________________,\n",
        "    loss=__________________________,\n",
        "    metrics=__________________________\n",
        ")\n",
        "\n",
        "# TODO: MNIST 데이터셋을 로드하고 전처리하세요. (데이터를 (784,)로 변환 및 정규화)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = __________________________\n",
        "x_test = __________________________\n",
        "\n",
        "# TODO: 모델을 학습하세요. (배치 크기: 64, 에포크: 5, 검증 데이터: 20%)\n",
        "history = transfer_model.fit(\n",
        "    __________________________,\n",
        "    batch_size=________________________,\n",
        "    epochs=________________________,\n",
        "    validation_split=________________________\n",
        ")\n",
        "\n",
        "# TODO: 모델을 평가하고 테스트 정확도를 출력하세요.\n",
        "test_loss, test_acc = __________________________\n",
        "print(f\"테스트 정확도: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "-enNfzYa9HRE"
      },
      "id": "-enNfzYa9HRE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}